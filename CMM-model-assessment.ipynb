{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment for Prior Authorization Classifiers\n",
    "While we are interested in models that perform well relative to quantitative measures, we would also like our model to perform well on independent test data. In other words, we seek a model that generalizes well to other data. \n",
    "\n",
    "We will look at bias/variance and confidence intervals for our metrics of interest (accuracy, etc).\n",
    "\n",
    "First we load the data and introduce the models of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load and clean the data. Then we merge the 3 dataframes into one and then split the resulting dataframe into two. \n",
    "In the first group, we have the data corresponding to pharmacy fills in which a PA was not requested and in the second we have the data corresponding to prescriptions for which a PA was requested.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_date=pd.read_csv(\"data/dim_date.csv\")\n",
    "df_claim=pd.read_csv(\"data/dim_claims.csv\")\n",
    "df_pa=pd.read_csv(\"data/dim_pa.csv\")\n",
    "df_bridge=pd.read_csv(\"data/bridge.csv\")\n",
    "\n",
    "# Clean data so all reject_code values are integers\n",
    "df_claim['reject_code'] = df_claim.reject_code.fillna(0).astype(int)\n",
    "\n",
    "# Merge the data frames\n",
    "df_main = pd.merge(df_claim, df_bridge, on='dim_claim_id')\n",
    "df_main = pd.merge(df_main, df_pa, how='left', on='dim_pa_id')\n",
    "df_main = pd.merge(df_main, df_date, how='left', on='dim_date_id')\n",
    "\n",
    "# split the data frames into two -- PA requested or not\n",
    "df_main_wPA = df_main[~np.isnan(df_main.pa_approved)].copy()\n",
    "df_main_noPA = df_main[np.isnan(df_main.pa_approved)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the dataframe corresponding to pharmacy fills with no PA requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_noPA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data frame corresponding to pharmacy fills with a PA requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_wPA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use one-hot encoding to turn the categorical features into binary features. There are two categorical features; the bin i.e. the payer and the drug type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all categorical features in the case that a PA was requested \n",
    "df_aug=df_main_wPA.copy()\n",
    "df_aug['70'] = pd.get_dummies(df_aug['reject_code'])[70]\n",
    "df_aug['75'] = pd.get_dummies(df_aug['reject_code'])[75]\n",
    "df_aug['76'] = pd.get_dummies(df_aug['reject_code'])[76]\n",
    "df_aug['bin417380']=pd.get_dummies(df_aug['bin'])[417380]\n",
    "df_aug['bin999001']=pd.get_dummies(df_aug['bin'])[999001]\n",
    "df_aug['bin417740']=pd.get_dummies(df_aug['bin'])[417740]\n",
    "df_aug['bin417614']=pd.get_dummies(df_aug['bin'])[417614]\n",
    "df_aug['drug_A']=pd.get_dummies(df_aug['drug'])['A']\n",
    "df_aug['drug_B']=pd.get_dummies(df_aug['drug'])['B']\n",
    "df_aug['drug_C']=pd.get_dummies(df_aug['drug'])['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the train-test split. We keep $20$% of the data as the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keep all except temporal features for now \n",
    "X=df_aug[['70', '75', '76', 'bin417380', 'bin999001','bin417740', 'bin417614','correct_diagnosis', 'contraindication', 'tried_and_failed', 'drug_A', 'drug_B','drug_C']]\n",
    "y=df_aug[['pa_approved']]\n",
    "X_train_gen,X_test_gen,y_train_gen,y_test_gen = train_test_split(X,y,\n",
    "                                                test_size=.2,\n",
    "                                                shuffle=True,\n",
    "                                                stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models of Interest\n",
    "We will analyze the following classifiers: \n",
    "1. logistic regression;\n",
    "2. Bayes classifier;\n",
    "3. random forests;\n",
    "4. decision trees;\n",
    "5. linear support vector machine; \n",
    "6. AdaBoost. \n",
    "\n",
    "We considered the voting classifier in some of the previous analyses. However, the analysis done here is computationally expensive and since the voting classifier performed as well as some of the other models, we choose to leave it out. \n",
    "\n",
    "We first import the necessary classes for the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "class GroupAverageClassifier(BaseEstimator, TransformerMixin, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.approved_count_ = Counter()\n",
    "        self.total_count_ = Counter()\n",
    "        for r, t in zip(X, y):\n",
    "            g = tuple(r)\n",
    "            self.approved_count_[g] += t\n",
    "            self.total_count_[g] += 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.predict_proba(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.predict_prob(X)\n",
    "        pred = (prob > 0.5).astype(int)\n",
    "        return pred\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        prob = np.zeros(X.shape[0])\n",
    "        eps = 1e-4\n",
    "        for i, r in enumerate(X):\n",
    "            g = tuple(r)\n",
    "            prob[i] = self.approved_count_[g] / (self.total_count_[g] + eps)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics of Interest \n",
    "We will be interested in accuracy, recall, precision, f1 and the area under the roc curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func_name(f):\n",
    "    name = f.__name__\n",
    "    if name.endswith('_score'):\n",
    "        name = name[:-6]\n",
    "    return name\n",
    "\n",
    "default_scores = [accuracy_score, precision_score, recall_score,f1_score, roc_auc_score]\n",
    "default_names = [get_func_name(f) for f in default_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "We will successively add in features for the models to be trained on and compare the metrics with each addition. Using the features corresponding to payer, rejection code, tried_and_failed, correct_diagnosis, contraindication and drug type, we test the importance of each feature for the above models. This will determine the order in which we successively add features to the models.\n",
    "\n",
    "See `pa_classifier_feature_importance.ipynb` for a more in-depth analysis of feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=['70', '75', 'bin417380','bin417740', 'bin417614','correct_diagnosis', 'contraindication', 'tried_and_failed', 'drug_A', 'drug_B']\n",
    "X_train=np.array(X_train_gen[feats])\n",
    "y_train=np.array(y_train_gen).ravel()\n",
    "X_test=np.array(X_test_gen[feats])\n",
    "y_test=np.array(y_test_gen).ravel()\n",
    "\n",
    "\n",
    "models = {}\n",
    "\n",
    "DATASETS = [(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "def evaluate(model, name, decimal=4, score_funcs=default_scores):\n",
    "    model.fit(X_train, y_train)\n",
    "    scores = np.empty((2, len(score_funcs)))\n",
    "    for i, (X, y) in enumerate(DATASETS):\n",
    "        yhat = model.predict(X)\n",
    "        for j, func in enumerate(score_funcs):\n",
    "            scores[i, j] = func(y, yhat)\n",
    "    scores = scores.round(decimal)\n",
    "    func_names = [get_func_name(f) for f in score_funcs]\n",
    "    df = pd.DataFrame(data=scores, columns=func_names, index=['train', 'test'])\n",
    "    \n",
    "    models[name] = (model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate all of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(penalty='l2', C=1)\n",
    "evaluate(lrc, 'Logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gac = GroupAverageClassifier()\n",
    "evaluate(gac, 'Group Avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "evaluate(rfc, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(penalty='l2', C=1)\n",
    "evaluate(svc, 'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators = 10,\n",
    "            algorithm=\"SAMME.R\",\n",
    "            learning_rate = 0.5)\n",
    "evaluate(ada_clf, 'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "evaluate(dt_clf, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the permutation importance test. Recall that the values for each feature correspond to the decrease in the model's performance when the training data for that feature is randomly shuffled. In this case, we use the default score (accuracy) as the metric, so the values below represent the change in accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "importances = []\n",
    "model_names = []\n",
    "std_devs=[]\n",
    "for name, (mdl, _) in models.items():\n",
    "    model_names.append(name)\n",
    "    r = permutation_importance(mdl, X_train, y_train, n_repeats=10,random_state=42)\n",
    "    importances.append(r['importances_mean'])\n",
    "    std_devs.append(r['importances_std'])\n",
    "df_importances = pd.DataFrame(data=importances,columns=feats,index=model_names)\n",
    "df_std_devs=pd.DataFrame(data=std_devs,columns=feats,index=model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "df_importances.style.background_gradient(cmap=cm, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, we can construct a list of all the features in order of descending importance for each model. This will be the list from which we add features to the models and compute the new metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features= list(df_importances.sort_values(by='Random Forest', axis=1, ascending=False).columns)\n",
    "log_features=list(df_importances.sort_values(by='Logistic', axis=1, ascending=False).columns)\n",
    "svc_features=list(df_importances.sort_values(by='SVC', axis=1, ascending=False).columns)\n",
    "bayes_features=list(df_importances.sort_values(by='Group Avg', axis=1, ascending=False).columns)\n",
    "ada_features=list(df_importances.sort_values(by='AdaBoost', axis=1, ascending=False).columns)\n",
    "dt_features=list(df_importances.sort_values(by='Decision Tree', axis=1, ascending=False).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias, Variance and Model Complexity\n",
    "Let $X$ be a feature vector, $Y$ a scalar response variable and $\\epsilon$ a random variable with $\\mathbb E(\\epsilon) = 0$ and $\\text{Var}(\\epsilon) = \\sigma^2$. If the relationship between $X$ and $Y$ is given by $Y = f(X) + \\epsilon$, we can derive an expression for the expected prediction error of a model $\\hat f$ for this relationship at feature $X = x_0$ using squared error loss as \n",
    "\\begin{align*} \\text{Err}(x_0) & = \\mathbb E \\left[\\left(Y - \\hat f(x_0)^2 \\right) \\ | \\ X = x_0  \\right] \\\\\n",
    "& = \\sigma^2 + \\left[ \\mathbb E \\hat f(x_0) - f(x_0) \\right]^2 + \\mathbb E \\left[ \\hat f(x_0) - \\mathbb E \\hat f(x_0) \\right]^2 \\\\\n",
    "& = \\sigma^2 + \\text{Bias}^2 \\left(\\hat f(x_0) \\right) + \\text{Var} \\left(\\hat f(x_0) \\right) \\\\\n",
    "& = \\text{Irreducible Error} + \\text{Bias}^2 + \\text{Variance}.\n",
    "\\end{align*}\n",
    "The first term is the variance of the truth $f(x_0)$ around its true mean and cannot be made smaller. The $\\text{Bias}^2$ term is the amount by which the average of our estimate differs from the true mean and the variance is the expected squared deviation of the model $\\hat f(x_0)$ around its mean. \n",
    "\n",
    "In general, as the complexity of the model $\\hat f$ increases, the squared bias will decrease and the variance will increase.\n",
    "\n",
    "We will compute the bias, variance, and expected prediction error for each model as features are successively added. \n",
    "\n",
    "First we import the necessary packages and write the function that will plot the bias-variance decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_variance_decomp(bias, variance, loss, model):\n",
    "    N = np.shape(bias)[1]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    x=range(1,N+1)\n",
    "\n",
    "    plt.plot(x, bias.reshape((-1,)),'b-o', label=\"Average Bias\", alpha=.5)\n",
    "    plt.plot(x, variance.reshape((-1,)),'r-o', label=\"Average Variance\", alpha=.5)\n",
    "    plt.plot(x, loss.reshape((-1,)),'g-o', label=\"Expected Prediction Error\", alpha=.5)\n",
    "\n",
    "    plt.xlabel(\"Number of Features\",fontsize=16)\n",
    "    plt.title('Bias-Variance Decomposition for PA Prediction via {}'.format(model))\n",
    "    plt.xticks(np.arange(0, N+2, step=1.0))\n",
    "\n",
    "    plt.legend(fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_var_loss(model,features):\n",
    "    \n",
    "    N=len(features)\n",
    "    losses= np.zeros((1,N))\n",
    "    biases=np.zeros((1,N))\n",
    "    variances=np.zeros((1,N))\n",
    "    \n",
    "    for i in range(0,N):\n",
    "        model_clone = clone(model)\n",
    "        new_feats=features[0:i+1]\n",
    "        X_train=np.array(X_train_gen.loc[:,new_feats])\n",
    "        y_train=np.array(y_train_gen).ravel()\n",
    "\n",
    "        X_test=np.array(X_test_gen.loc[:,new_feats])\n",
    "        y_test=np.array(y_test_gen).ravel()\n",
    "        losses[0,i],biases[0,i],variances[0,i]=\\\n",
    "        bias_variance_decomp(model,X_train,y_train,X_test,y_test,loss='0-1_loss',num_rounds=200)\n",
    "        \n",
    "    return losses, biases, variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier()\n",
    "tree_losses, tree_biases, tree_variances=get_bias_var_loss(forest_clf,rf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_variance_decomp(tree_biases, tree_variances, tree_losses, 'Random Forests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance is very low and increases once $6$ features are added. Because the variance is so low, the bias and expected prediction error visually coincide on this plot but when we print the values, we see that they do not coincide. The differences between the two are too small to be apparent from this plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Biases:',tree_biases)\n",
    "print('Expected Prediction Error', tree_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_losses, log_biases, log_variances=get_bias_var_loss(log_reg,log_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_bias_variance_decomp(log_biases, log_variances, log_losses, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance, bias, and expected prediction error decreases as the first several features are added and then start to increase when $7$ or $8$ features are added. Again, the expected prediction error and the bias are too close to be visually distinct on this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(C=1)\n",
    "svc_losses, svc_biases, svc_variances=get_bias_var_loss(svc,svc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_variance_decomp(svc_biases, svc_variances, svc_losses, 'Linear Support Vector Machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias, variance, and expected prediction error all decrease when the first several features are added and start to increase slowly once $9$ or $10$ features are added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_clf=GroupAverageClassifier()\n",
    "bayes_losses, bayes_biases, bayes_variances=get_bias_var_loss(bayes_clf,bayes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_variance_decomp(bayes_biases, bayes_variances, bayes_losses, 'Bayes Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance for the Bayes classifier is very close to zero and starts to increase once $9$ features in the training data are used. The bias and expected prediction error start to decrease at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Variance: ', bayes_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators = 10,\n",
    "            algorithm=\"SAMME.R\",\n",
    "            learning_rate = 0.5\n",
    "        )\n",
    "ada_losses, ada_biases, ada_variances=get_bias_var_loss(ada_clf,ada_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_variance_decomp(ada_biases, ada_variances, ada_losses, 'AdaBoost Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average bias, average variance, and expected prediction error decreases with the first several added features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_losses, dt_biases, dt_variances=get_bias_var_loss(dt_clf,dt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_variance_decomp(dt_biases, dt_variances, dt_losses, 'Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average variance decreases sharply once a second feature is added to the training data and starts to increase again once $7$ features are added. \n",
    "### Conclusions\n",
    "Often, when plotting average bias, average variance and expected prediction error against model complexity, there is a clear picture of the bias variance trade off. In our analysis, this does not seem to be the case. It could be that the change in complexity of the models was not large enough to capture this change -- adding features to the training data is one way to increase complexity, but so is changing other hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "This is a good method for assessing statistical accuracy. Similar to cross-validation, bootstap methods seek to estimate the expected prediction error Err and other measures of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Bootstrap Estimates \n",
    "The general procedure for standard bootstrapping is as follows: \n",
    "1. make the train-test split so that the training set is of size N;\n",
    "2. sample with replacement from the training set N times to create a bootstrapped data set;\n",
    "3. train the model on the bootstrapped training data;\n",
    "4. compute the error (or other measure of performance) of the model classification on the training set.\n",
    "\n",
    "If we repeat this process B times, we can take the middle $k\\%$ of the performance measures to construct a confidence interval. This method will give us a confidence interval about the measure of performance on the training set.\n",
    "\n",
    "In this section, we repeat the process $100$ times to construct $95\\%$ confidence intervals around accuracy, precision, etc. It would be better to do more repetitions, but the computational expense is prohibitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def get_func_name(f):\n",
    "    name = f.__name__\n",
    "    if name.endswith('_score'):\n",
    "        name = name[:-6]\n",
    "    return name\n",
    "\n",
    "default_scores = [accuracy_score, precision_score, recall_score,f1_score, roc_auc_score]\n",
    "default_names = [get_func_name(f) for f in default_scores]\n",
    "\n",
    "# get the bootstrapped scores for a model. \n",
    "def get_bootstrapped_scores(model, X_train, y_train, n_bootstrap=100, scores=default_scores):\n",
    "    score_data=np.empty((n_bootstrap, len(scores)))\n",
    "    for i in range(0, n_bootstrap):\n",
    "        X_boot, y_boot=resample(np.array(X_train), np.array(y_train).ravel(), replace=True,random_state=i)\n",
    "        cloned_model=clone(model)\n",
    "        cloned_model.fit(X_boot, y_boot)\n",
    "        preds=cloned_model.predict(X_boot)\n",
    "        for j, func in enumerate(scores):\n",
    "            score_data[i, j] = func(y_boot, preds)\n",
    "        if i%10 == 0: \n",
    "            print('iteration', i)\n",
    "    func_names = [get_func_name(f) for f in scores]\n",
    "    df = pd.DataFrame(data=score_data, columns=func_names)\n",
    "    return df \n",
    "\n",
    "# this function computes metrics on bootstrapped samples for a changing list of features \n",
    "def seq_bootstrapped_scores(model, model_feats, X_gen, y_gen, scores=default_scores):\n",
    "    cols = [get_func_name(f) for f in scores]\n",
    "    cols.append(\"model_complexity\")\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    for i,j in enumerate(model_feats):\n",
    "        total_feats=model_feats[0:i+1]\n",
    "        X,y =X_gen.loc[:,total_feats], y_gen\n",
    "        i_scores=get_bootstrapped_scores(model, X,y)\n",
    "        i_scores['model_complexity'] = i+1\n",
    "        df=pd.concat([df, i_scores], axis= 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-bag Error \n",
    "We use bootstrapping to build confidence intervals around the test error. Previously, we were building confidence intervals around the training error. The procedure for out-of-bag bootstrapping is as follows: \n",
    "1. make the train-test split so that the training set is of size $N$;\n",
    "2. sample with replacement from the training set $N$ times to make a new training set;\n",
    "3. any unsampled data from the original training set becomes the test set;\n",
    "4. get measures of performance on this test set.\n",
    "\n",
    "Below we write the functions that perform out-of-bag bootstrapping. We also write the functions to build the confidence intervals and plot them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import BootstrapOutOfBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_boots_oob_scores(model, X, y, n_bootstrap=100, scores=default_scores):\n",
    "    score_data=np.empty((n_bootstrap, len(scores)))\n",
    "    oob=BootstrapOutOfBag(n_splits=n_bootstrap, random_seed=101)\n",
    "    N=len(X)\n",
    "    i=0\n",
    "    for train_split, test_split in oob.split(np.array(range(0,N))):\n",
    "        X_train =np.array(X.iloc[train_split])\n",
    "        y_train= np.array(y.iloc[train_split]).ravel()\n",
    "        X_test, y_test=X.iloc[test_split], y.iloc[test_split]\n",
    "        \n",
    "        cloned_model=clone(model)\n",
    "        cloned_model.fit(X_train, y_train)\n",
    "        test_preds=cloned_model.predict(X_test)\n",
    "        for j, func in enumerate(scores):\n",
    "            score_data[i, j] = func(y_test, test_preds)\n",
    "            func_names = [get_func_name(f) for f in scores]\n",
    "            df = pd.DataFrame(data=score_data, columns=func_names)\n",
    "        i+=1\n",
    "        if i%10 == 0: \n",
    "            print('iteration', i)\n",
    "    return df\n",
    "\n",
    "def seq_oob_scores(model, model_feats, X_gen, y_gen, scores=default_scores):\n",
    "    cols = [get_func_name(f) for f in scores]\n",
    "    cols.append(\"model_complexity\")\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    for i,j in enumerate(model_feats):\n",
    "        total_feats=model_feats[0:i+1]\n",
    "        X,y =X_gen.loc[:,total_feats], y_gen\n",
    "        i_scores=get_boots_oob_scores(model, X,y)\n",
    "        i_scores['model_complexity'] = i+1\n",
    "        df=pd.concat([df, i_scores], axis= 0)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the data frame will contain all score types (accuracy, prec, etc) that's returned from get_bootstrapped_scores() \n",
    "def create_ci(df, alpha, scores=default_scores):\n",
    "    new_df = pd.DataFrame(columns=['lower_bound', 'middle', 'upper_bound', 'model_complexity', 'score_type'])\n",
    "    x=range(1,df[['model_complexity']].nunique()[0]+1)\n",
    "    num_scores=np.shape(df)[1]-1    \n",
    "    for level in x:\n",
    "        # extract bootstrap samples cooresponding to model complexity level and then drop that column so only the data remains\n",
    "        con_ints=np.zeros((num_scores,4))\n",
    "        df_small=df[df['model_complexity']==level].drop('model_complexity', axis = 1)\n",
    "        for i,j in enumerate(df_small.columns):\n",
    "            data=np.array(df_small[j])\n",
    "            p = ((1.0-alpha)/2.0) * 100\n",
    "            con_ints[i,0] = max(0.0, np.percentile(data, p))\n",
    "            p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "            con_ints[i,1]=np.percentile(data, 50)\n",
    "            con_ints[i,2] = min(1.0, np.percentile(data, p))\n",
    "            con_ints[:,3]=level\n",
    "            \n",
    "        func_names = [get_func_name(f) for f in scores]\n",
    "        level_df = pd.DataFrame(data=con_ints, columns=['lower_bound', 'middle', 'upper_bound', 'model_complexity'])\n",
    "        level_df['score_type']=func_names\n",
    "        new_df=pd.concat([new_df, level_df], axis=0)\n",
    "    return new_df\n",
    "\n",
    "# the data frames have a column for each score and a column for the model complexity \n",
    "\n",
    "def plot_ci(df, model, set_type, features, scores=default_names):\n",
    "    N = df[['model_complexity']].nunique()[0] # types of scores\n",
    "    x=range(1,N+1)\n",
    "    fig, ax = plt.subplots(figsize=(14,10))\n",
    " \n",
    "    colors=['tab:blue', 'tab:red', 'tab:green', 'tab:orange', 'tab:purple']\n",
    "    for i,j in enumerate(scores):\n",
    "        data=df[df['score_type']==j]\n",
    "        midvals=np.array(data[['middle']])\n",
    "        ub=np.array(data[['upper_bound']])\n",
    "        lb=np.array(data[['lower_bound']])\n",
    "        ax.plot(features, np.reshape(midvals,(-1,)), 'o-', color=colors[i])\n",
    "        ax.plot(features, np.reshape(ub,(-1,)), '-',alpha=.2, color=colors[i])\n",
    "        ax.plot(features, np.reshape(lb,(-1,)), '-',alpha=.2, color=colors[i])\n",
    "        ax.fill_between(features, np.reshape(lb,(-1,)), np.reshape(ub,(-1,)), alpha=.2, label=j, color=colors[i])\n",
    "\n",
    "    ax.set_xlabel(\"Features Added\",fontsize=16)\n",
    "    ax.set_title('Confidence Intervals for PA Prediction on the {} set via {}'.format(set_type,model))\n",
    "    ax.tick_params(axis='x', rotation=70)\n",
    "    ax.set_xticks(np.arange(0, N+3, step=1.0))\n",
    "   \n",
    "\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set alpha here. This determines the level of the confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "We first take $100$ bootstrap samples from the data and train the random forest classifier on these samples. We must do this for all combinations of features we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_b_scores=seq_bootstrapped_scores(rf_clf, rf_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_b_ci=create_ci(rf_b_scores, alpha)\n",
    "plot_ci(rf_b_ci, 'Random Forest', 'Training', rf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points on the above plot correspond to the median of the confidence interval and the opaque bars correspond to the endpoints of the interval. Intuitively, the narrower the shaded region about a point, the less variability there is in that score. \n",
    "\n",
    "The x-axis describes the features that are added. For example, the points in the column titled 'contraindication' correspond to the measures of performance for the random forest classifier trained on only the feature 'contraindication'. In the next column, titled '70', the points correspond to the measures of performance for the random forest classifier trained on the features ['contraindication', '70']. Thus, as we move from left to right on the $x$-axis, the training set accumulates features. \n",
    "\n",
    "We see that there is a significant amount of variability in the scores once $5$ or $6$ features are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_o_scores=seq_oob_scores(rf_clf, rf_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_o_ci=create_ci(rf_o_scores, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to know if there is a significant difference between the scores on the training set and the test set. Because we fixed the random state in the bootstrap splits, we can look at the difference between the scores and extract the largest values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The largest difference between the enpoints or middle of confidence intervals on the bootstrap training set and test set are:')\n",
    "print()\n",
    "print(rf_o_ci.drop(['score_type'], axis=1).subtract(rf_b_ci.drop(['score_type'], axis=1)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these differences are small, we will focus on plotting the scores from the regular bootstrap samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_b_scores=seq_bootstrapped_scores(log_reg, log_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_b_ci=create_ci(log_b_scores, alpha)\n",
    "plot_ci(log_b_ci, 'Logistic Regression', 'Training', log_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier performs inconsistently when it is trained on only $1$ feature, which is not surprising. The measures of performance are very consistent until we add 'tried_and_failed' as a feature to the training data. Then recall, the area under the ROC curve and the precision have a larger variability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_o_scores=seq_oob_scores(log_reg, log_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_o_ci=create_ci(log_o_scores, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again check the difference between the midpoints and endpoints of the confidence intervals constructed for the test set and the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The largest difference between the endpoints or middle of confidence intervals on the bootstrap training set and test set are:')\n",
    "print()\n",
    "print(log_o_ci.drop(['score_type'], axis=1).subtract(log_b_ci.drop(['score_type'], axis=1)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences are negligible, similar to the case of the random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(penalty='l2', C=1)\n",
    "svc_b_scores=seq_bootstrapped_scores(svc, svc_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_b_ci=create_ci(svc_b_scores, alpha)\n",
    "plot_ci(svc_b_ci, 'Support Vector Machine', 'Training', svc_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear support vector machine performs the most consistently of all the models we have looked at so far. We see that the performance is inconsistent when the classifier is trained only on the feature '70', corresponding to the reject code. Additionally, there is some  more variability once we add 8 features. \n",
    "\n",
    "We take a closer look below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_b_ci[(svc_b_ci['model_complexity']>8) & (svc_b_ci['score_type']=='roc_auc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the width of these confidence intervals is about ~$0.008$ and is small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(penalty='l2', C=1)\n",
    "svc_o_scores=seq_oob_scores(svc, svc_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_o_ci=create_ci(svc_o_scores, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The largest difference between the endpoints or middle of confidence intervals on the bootstrap training set and test set are:')\n",
    "print()\n",
    "print(svc_o_ci.drop(['score_type'], axis=1).subtract(svc_b_ci.drop(['score_type'], axis=1)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bayes_clf=GroupAverageClassifier()\n",
    "bayes_b_scores=seq_bootstrapped_scores(bayes_clf, bayes_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_b_ci=create_ci(bayes_b_scores, alpha)\n",
    "plot_ci(bayes_b_ci, 'Bayes Classifier', 'Training', bayes_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "The Bayes classifier also performs consistently, with visible variation once the $10$th feature is added. We can take a closer look at these confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_b_ci[bayes_b_ci['model_complexity']==10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The width of the largest confidence interval is $~0.03$, which corresponds to $3$%. Depending on the tolerance for error, this quantity is likely to be significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators = 10,\n",
    "            algorithm=\"SAMME.R\",\n",
    "            learning_rate = 0.5\n",
    "        )\n",
    "ada_b_scores=seq_bootstrapped_scores(ada_clf, ada_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_b_ci=create_ci(ada_b_scores, alpha)\n",
    "plot_ci(ada_b_ci, 'AdaBoost Classifier', 'Training', ada_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost performs inconsistently with $10$ estimators on the feature '70', which corresponds to the reject code. However, once more features are added, the performance of the AdaBoost classifier stabilizes and becomes consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators = 10,\n",
    "            algorithm=\"SAMME.R\",\n",
    "            learning_rate = 0.5\n",
    "        )\n",
    "ada_o_scores=seq_oob_scores(ada_clf, ada_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_o_ci=create_ci(ada_o_scores, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The largest difference between the endpoints or middle of confidence intervals on the bootstrap training set and test set are:')\n",
    "print()\n",
    "print(ada_o_ci.drop(['score_type'], axis=1).subtract(ada_b_ci.drop(['score_type'], axis=1)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_b_scores=seq_bootstrapped_scores(dt_clf, dt_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_b_ci=create_ci(dt_b_scores, alpha)\n",
    "plot_ci(dt_b_ci, 'Decision Tree Classifier', 'Training', dt_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the performance of the decision tree is similar to that of the random forest. \n",
    "\n",
    "Consider the following sequence combinations corresponding to the smallest confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_features[:2])\n",
    "print(dt_features[:3])\n",
    "print(dt_features[:4])\n",
    "print(dt_features[:6])\n",
    "print(dt_features[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is surprising that the following feature combination, gives more inconsistent results, given that the last two sets of features above contain it as a proper subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_o_scores=seq_oob_scores(dt_clf, dt_features, X_train_gen, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_o_ci=create_ci(dt_o_scores, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The largest difference between the endpoints or middle of confidence intervals on the bootstrap training set and test set are:')\n",
    "print()\n",
    "print(dt_o_ci.drop(['score_type'], axis=1).subtract(dt_b_ci.drop(['score_type'], axis=1)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best of: Maximizing the ROC Score\n",
    "We want to choose the model with the highest ROC score and an accuracy that is greater than $0.73$, since that is the accuracy of the baseline model that predicts all PAs will be approved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the Bootstrap\n",
    "First, we choose the models from the above discussion that maximize the ROC curve and plot the confidence intervals of the performance measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model\n",
    "rf_best=rf_b_ci[(rf_b_ci['score_type']=='roc_auc') & (rf_b_ci['model_complexity'] > 1)].sort_values(by='lower_bound', ascending=False).iloc[0]\n",
    "log_best=log_b_ci[(log_b_ci['score_type']=='roc_auc') & (log_b_ci['model_complexity'] > 1)].sort_values(by='lower_bound', ascending=False).iloc[0]\n",
    "bayes_best=bayes_b_ci[(bayes_b_ci['score_type']=='roc_auc')& (bayes_b_ci['model_complexity'] > 1) ].sort_values(by='lower_bound', ascending=False).iloc[0]\n",
    "ada_best=ada_b_ci[(ada_b_ci['score_type']=='roc_auc') & (ada_b_ci['model_complexity'] > 1)].sort_values(by='lower_bound', ascending=False).iloc[0]\n",
    "dt_best=dt_b_ci[(dt_b_ci['score_type']=='roc_auc') & (dt_b_ci['model_complexity'] > 1)].sort_values(by='lower_bound', ascending=False).iloc[0]\n",
    "svc_best=svc_b_ci[(svc_b_ci['score_type']=='roc_auc') & (svc_b_ci['model_complexity'] > 1)].sort_values(by='lower_bound', ascending=False).iloc[0]\n",
    "\n",
    "data=np.zeros((6,4))\n",
    "\n",
    "data[0,:]=rf_best[0:4]\n",
    "data[1,:]=log_best[0:4]\n",
    "data[2,:]=bayes_best[0:4]\n",
    "data[3,:]=ada_best[0:4]\n",
    "data[4,:]=dt_best[0:4]\n",
    "data[5,:]=svc_best[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features from the best models \n",
    "best_feats=[]\n",
    "best_feats.append(rf_features[:int(rf_best[3])])\n",
    "best_feats.append(log_features[:int(log_best[3])])\n",
    "best_feats.append(bayes_features[:int(bayes_best[3])])\n",
    "best_feats.append(ada_features[:int(ada_best[3])])\n",
    "best_feats.append(dt_features[:int(dt_best[3])])\n",
    "best_feats.append(svc_features[:int(svc_best[3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the ROC AUC score for each of the best models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[ 'Random Forest','Logistic Regression', 'Bayes Classifier','AdaBoost', 'Decision Tree','Support Vector Machine']\n",
    "x=range(1,len(model_list)+1)\n",
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "for i in x:\n",
    "    ax.plot(i, data[i-1,1], 'o')\n",
    "    vals=(i-.01, i+.025)\n",
    "    ax.fill_between(vals, data[i-1,0], data[i-1,2], alpha=.2, label=model_list[i-1])\n",
    "\n",
    "ax.set_ylabel('ROC AUC Score')\n",
    "\n",
    "print('FEATURES USED:')\n",
    "print()\n",
    "print('Random Forest: ', best_feats[0])\n",
    "print('Logistic Regression: ', best_feats[1])\n",
    "print('Bayes Classifier: ', best_feats[2])\n",
    "print('AdaBoost: ', best_feats[3])\n",
    "print('Decision Tree: ', best_feats[4])\n",
    "print('Linear Support Vector Machine: ', best_feats[5])\n",
    "\n",
    "ax.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somewhat surprising that the confidence intervals are all of similar size and there seems to be a separation of the models into two groups. One contains the random forest, logistic regression, decision tree and AdaBoost while the other contains the Bayes classifier and the support vector machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Fine Tuning with Feature Selection\n",
    "Choosing to sequentially add features in order of importance does not necessarily yield in optimal results.  Therefore, we use the fine tuning done in PA_classifier.ipynb to choose our features and parameters. This notebook computes the measures of performance for each model using all possible feature combinations. The fine tuned features used below are found in the notebooks 'pa_classifier.ipynb' and 'feature_selection.ipynb'.\n",
    "\n",
    "Note that because we use the previous functions written for confidence intervals in which the model complexity might be chaanging, we must include a model complexity value. We choose to set 'model_complexity'=1, but this is a dummy variable and has no significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "We first train the classifier on the optimal set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_rf_feats=['70', '75', 'bin417380','bin417740', 'bin417614','correct_diagnosis', 'tried_and_failed', 'drug_A', 'drug_B']\n",
    "\n",
    "rf_clf= RandomForestClassifier(criterion= 'gini', max_depth = 7, min_samples_split= 2, n_estimators=15)\n",
    "rf_best_boot=get_bootstrapped_scores(rf_clf, X_train_gen[fine_tune_rf_feats], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence intervals for the scores are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_ci=create_ci(rf_best_boot, alpha=.95)\n",
    "rf_best_ci['model_name']='Random Forest'\n",
    "rf_best_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree \n",
    "We first train the decision tree on the optimal features. Note that this set of features corresponds to the set of features used for the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_dt_feats=['70', '75', 'bin417380','bin417740', 'bin417614','correct_diagnosis', 'tried_and_failed', 'drug_A', 'drug_B']\n",
    "\n",
    "dt_clf= DecisionTreeClassifier(criterion= 'gini', max_depth = 7, min_samples_split= 2)\n",
    "dt_best_boot=get_bootstrapped_scores(dt_clf, X_train_gen[fine_tune_dt_feats], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_boot['model_complexity']=1\n",
    "dt_best_ci=create_ci(dt_best_boot, alpha=.95)\n",
    "dt_best_ci['model_name']='Decision Tree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Support Vector Machine\n",
    "We train the linear support vector machine on its best features for the ROC curve as well and find the confidence intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_svc_feats=['70', '75', 'bin417614', 'tried_and_failed', 'drug_B']\n",
    "svc_clf = LinearSVC(C=1)\n",
    "svc_best_boot=get_bootstrapped_scores(svc_clf, X_train_gen[fine_tune_svc_feats], y_train)\n",
    "svc_best_boot['model_complexity']=1\n",
    "svc_best_ci=create_ci(svc_best_boot, alpha=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best_ci['model_name']='Support Vector Machine'\n",
    "svc_best_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression \n",
    "We train the logistic regression model on its best features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_log_feats=['70', 'bin417614', 'tried_and_failed', 'drug_B']\n",
    "log_clf = LogisticRegression()\n",
    "log_best_boot=get_bootstrapped_scores(log_clf, X_train_gen[fine_tune_log_feats], y_train)\n",
    "log_best_boot['model_complexity']=1\n",
    "\n",
    "log_best_ci=create_ci(log_best_boot, alpha=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_best_ci['model_name']='Logistic Regression'\n",
    "log_best_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine_tune_ada_feats=['70', '75', 'drug_B']\n",
    "fine_tune_ada_feats=['70', '75', 'bin417380', 'contraindication', 'drug_A', 'drug_B']\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators = 10,\n",
    "            algorithm=\"SAMME.R\",\n",
    "            learning_rate = 0.5\n",
    "        )\n",
    "\n",
    "ada_best_boot=get_bootstrapped_scores(ada_clf, X_train_gen[fine_tune_ada_feats], y_train)\n",
    "ada_best_boot['model_complexity']=1\n",
    "ada_best_ci=create_ci(ada_best_boot, alpha=.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_best_ci['model_name']='AdaBoost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes Classifier\n",
    "Lastly, we train the Bayes classifier on its optimal set of features. It is interesting to note that 'contraindication' only appears in the feature list for AdaBoost. This seems interesting because this feature corresponds to whether the treatment could be harmful to the patient, which seems important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_bayes_feats=['75', 'bin417380', 'bin417740', 'bin417614', 'tried_and_failed', 'drug_B']\n",
    "bayes_clf=GroupAverageClassifier()\n",
    "\n",
    "bayes_best_boot=get_bootstrapped_scores(bayes_clf, X_train_gen[fine_tune_bayes_feats], y_train)\n",
    "bayes_best_boot['model_complexity']=1\n",
    "bayes_best_ci=create_ci(bayes_best_boot, alpha=.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_best_ci['model_name']='Bayes Classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Results\n",
    "First we collect all of our results into a single data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6 # number of models\n",
    "ci_list=[rf_best_ci, dt_best_ci, log_best_ci, svc_best_ci, ada_best_ci, bayes_best_ci]\n",
    "\n",
    "df=rf_best_ci.append(dt_best_ci).append(log_best_ci).append(svc_best_ci).append(ada_best_ci).append(bayes_best_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "model_names=['Random Forest', 'Decision Tree', 'Logistic Regression','Support Vector Machine', 'AdaBoost', 'Bayes Classifier']\n",
    "\n",
    "colors=['tab:blue', 'tab:red', 'tab:green', 'tab:orange', 'tab:purple']\n",
    "\n",
    "for i,j in enumerate(default_names):\n",
    "    \n",
    "    data=df[df['score_type']==j]\n",
    "    midvals=np.array(data[['middle']])\n",
    "    ub=np.array(data[['upper_bound']])\n",
    "    lb=np.array(data[['lower_bound']])\n",
    "    ax.plot(model_names, np.reshape(midvals,(-1,)), 'o-', color=colors[i])\n",
    "    ax.plot(model_names, np.reshape(ub,(-1,)), '-',alpha=.2, color=colors[i])\n",
    "    ax.plot(model_names, np.reshape(lb,(-1,)), '-',alpha=.2, color=colors[i])\n",
    "    ax.fill_between(model_names, np.reshape(lb,(-1,)), np.reshape(ub,(-1,)), alpha=.1, label=j, color=colors[i])\n",
    "\n",
    "    #ax.fill_between(x, np.reshape(lb,(-1,)), np.reshape(ub,(-1,)), alpha=.2, label=j, step='mid')\n",
    "    #ax.plot(x, np.reshape(midvals,(-1,)), 'o')\n",
    "ax.set_xlabel(\"Models\",fontsize=16)\n",
    "ax.set_title('Confidence Intervals for Models')\n",
    "ax.tick_params(axis='x', rotation=70)\n",
    "#ax.set_xticklabels(model_names)\n",
    "\n",
    "\n",
    "ax.legend(fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we see that the decision tree and the random forest have the largest confidence intervals for the ROC curve, the precision, and the f1 score. The other models have visible confidence intervals as well, but they are smaller. \n",
    "\n",
    "We can look at just a few of the scores types to get a better understanding of what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "model_names=['Random Forest', 'Decision Tree', 'Logistic Regression','Support Vector Machine', 'AdaBoost', 'Bayes Classifier']\n",
    "\n",
    "colors=['tab:blue', 'tab:red', 'tab:green', 'tab:orange', 'tab:purple']\n",
    "\n",
    "custom_names=['roc_auc', 'f1','recall']\n",
    "\n",
    "\n",
    "for x, name in enumerate(model_names):\n",
    "    model_data=df[df['model_name']==name]\n",
    "    for i,j in enumerate(custom_names):\n",
    "        data=model_data[model_data['score_type']==j]\n",
    "        midvals=np.array(data[['middle']])\n",
    "        ub=np.array(data[['upper_bound']])\n",
    "        lb=np.array(data[['lower_bound']])\n",
    "        ax.plot(x, np.reshape(midvals,(-1,)), 'o-', color=colors[i])\n",
    "        interval=(x-.01, x+.02)\n",
    "        if x == 0:\n",
    "            ax.fill_between(interval, np.reshape(lb,(-1,)), np.reshape(ub,(-1,)), \n",
    "                            alpha=.2, label=j, color=colors[i])\n",
    "        else:\n",
    "            ax.fill_between(interval, np.reshape(lb,(-1,)), np.reshape(ub,(-1,)), \n",
    "                            alpha=.2, color=colors[i])\n",
    "\n",
    "ax.set_xlabel(\"Models\",fontsize=16)\n",
    "ax.set_title('Confidence Intervals for Models')\n",
    "ax.tick_params(axis='x', rotation=70)\n",
    "ax.set_xticks(range(0,N))\n",
    "ax.set_xticklabels(model_names)\n",
    "\n",
    "\n",
    "ax.legend(fontsize=16, loc='lower right')\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig('conf_inf2.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can represent each score on its own plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr_names=['R. For.','D. Tree', 'Log. Reg.', 'SVM','AdaB.','Bayes']\n",
    "fig, axs = plt.subplots(3,2, figsize=(16,25))\n",
    "\n",
    "for j in range(0,2):\n",
    "    for k in range(0,3):\n",
    "        if 2*k+j==5: \n",
    "            fig.delaxes(axs[k,j])\n",
    "        else:    \n",
    "            score=default_names[2*k+j]\n",
    "            data=df[df['score_type']==score]\n",
    "            x = range(0, N)\n",
    "            for i in x:\n",
    "                axs[k,j].plot(i, np.array(data['middle'])[i], 'o')\n",
    "                vals=(i-.015, i+.03)\n",
    "                axs[k,j].fill_between(vals, np.array(data['lower_bound'])[i], np.array(data['upper_bound'])[i], alpha=.4, label=model_list[i-1], joinstyle='round')\n",
    "\n",
    "            axs[k,j].set_xticks(range(0,N))\n",
    "            axs[k,j].set_xticklabels(abbr_names)\n",
    "            axs[k,j].tick_params(axis='x', rotation=70)\n",
    "            axs[k,j].set_title(default_names[2*k+j])\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion \n",
    "First notice that the random forest and decision tree perform consistently better in the ROC score than AdaBoost. Since the solid dot represents the middle of the confidence interval, the above plot tells us that the random forest and decision tree outperform AdaBoost with respect to this metric more than $50$% of the time. Furthermore, the confidence intervals for the random forest and decision tree corresponding to recall and f score have a lower bound and median that is similar to that of the other models, but the upper bound is larger. Therefore, the random forest and decision tree models seem to be the best from this analysis. \n",
    "\n",
    "Finally, the permuation importance and drop feature importance allow us to conclude that the random forest is the best classifier. See pa_classifier_feature_importance.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
